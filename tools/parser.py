# parser/parser.py
import spacy

# è¼‰å…¥è‹±æ–‡ NLP æ¨¡å‹
nlp = spacy.load("en_core_web_sm")


def parse_sentence(text: str):
    """
    å°‡å¥å­æ–·è©ä¸¦è¼¸å‡ºåŸºæœ¬è©æ€§ã€èªæ„è³‡è¨Š
    """
    doc = nlp(text)
    tokens = []

    for token in doc:
        if not token.is_space:  # è·³éç©ºç™½ç¬¦è™Ÿ
            tokens.append(
                {
                    "text": token.text,
                    "lemma": token.lemma_,
                    "pos": token.pos_,
                    "tag": token.tag_,
                    "dep": token.dep_,
                    "head": token.head.text,
                }
            )

    return tokens


def extract_keywords(tokens, allowed_pos={"NOUN", "VERB", "ADJ"}):
    """
    å¾è§£æçµæœä¸­æ“·å–é‡è¦è©å½™ï¼ˆæ ¹å‹ lemmaï¼‰
    """
    keywords = []
    for t in tokens:
        if t["pos"] in allowed_pos and t["lemma"].isalpha():
            keywords.append(t["lemma"].lower())
    return list(set(keywords))  # å»é™¤é‡è¤‡


if __name__ == "__main__":
    sentence = "Freedom is a basic human right."
    results = parse_sentence(sentence)

    print(f"\nğŸ” åŸå§‹å¥å­ï¼š{sentence}\n")
    print("è§£æçµæœï¼š")
    print(f"{'Token':<15}{'Lemma':<15}{'POS':<8}{'Dep':<15}{'Head'}")
    print("-" * 60)
    for token in results:
        print(
            f"{token['text']:<15}{token['lemma']:<15}{token['pos']:<8}{token['dep']:<15}{token['head']}"
        )

    # æ“·å–é—œéµè©
    keywords = extract_keywords(results)
    print(f"\nğŸ”‘ æ“·å–å‡ºçš„é—œéµè©ï¼ˆå»ºè­°æŸ¥è©¢ï¼‰ï¼š{keywords}")

